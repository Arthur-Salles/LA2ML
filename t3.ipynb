{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import torchvision\n",
    "from torchvision.transforms import ToTensor, Normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from abc import ABC, abstractmethod\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II : Multinominal Softmax Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 784) (18000, 784) (42000, 10) (18000,)\n"
     ]
    }
   ],
   "source": [
    "raw_mnist = torchvision.datasets.MNIST(\"./\", download=True)\n",
    "\n",
    "x_train, x_test = train_test_split(raw_mnist.data, test_size=0.3, random_state=166003)\n",
    "y_train, y_test = train_test_split(raw_mnist.targets, test_size=0.3, random_state=166003)\n",
    "\n",
    "x_train = x_train.reshape(-1, 28 * 28).numpy()\n",
    "x_test = x_test.reshape(-1, 28 * 28).numpy()\n",
    "\n",
    "# important so softmax wont overflow \n",
    "x_train = (x_train - np.min(x_train)) / (np.max(x_train) - np.min(x_train))\n",
    "x_test = (x_test - np.min(x_test)) / (np.max(x_test) - np.min(x_test))\n",
    "\n",
    "\n",
    "y_test = y_test.numpy()\n",
    "y_train = y_train.numpy()\n",
    "\n",
    "# one hotting labels\n",
    "# y_test = np.eye(10)[y_test].astype(np.int32)\n",
    "y_train = np.eye(10)[y_train].astype(np.int32)\n",
    "\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradients \n",
    "\n",
    "\n",
    "Feed forward:\n",
    "$$ z = \\hat{y}= \\textbf{xw} + \\textbf{b} $$\n",
    "\n",
    "Softmax Activation Function:\n",
    "\n",
    "$$\\sigma (z) = \\frac{\\text{exp}(z_i)}{\\sum_{j=0}^{K}\\text{exp}(z_j)}$$\n",
    "$$\\sigma (w) = \\frac{\\text{exp}(xw_i+b_i)}{\\sum_{j=0}^{K}\\text{exp}(xw_j+b_j)}$$\n",
    "\n",
    "Loss Function - CrossEntropy for Binary Classification: \n",
    "\n",
    "$$ L(y, \\hat{y}) = âˆ’log \\hspace{2px} p(y|x) = -[y \\hspace{2px}log \\hspace{2px} \\hat{y} \\hspace{2px}+\\hspace{2px} (1-y)\\hspace{2px}log \\hspace{2px}(1-\\hat{y})]$$\n",
    "\n",
    "Considering the labels are represented in one-hot fashion, in which each element of $\\hat{y}$ represents the probability $ p(\\hat{y_k}= 1 |x)$, therefore the multiclass version will be the sum of the logs of the $K$ classes \n",
    "\n",
    "$$ L(y, \\hat{y}) = -\\sum_{i=0}^{K} y_i \\hspace{2px}log\\hspace{2px}\\hat{y_i}  $$ \n",
    "\n",
    "Note that, given the one hot encoding all terms that are not the desired class k, will be equal to zero, simplifying the loss to: \n",
    "\n",
    "$$ L(y, \\hat{y}) = -\\hspace{2px}log\\hspace{2px}\\hat{y_k}  $$\n",
    "\n",
    "$$ L(w) = -log\\hspace{2px} \\left(  \\frac{\\text{exp}(\\textbf{xw}_i + \\textbf{b}_i )}{\\sum_{j=0}^{K}\\text{exp}(\\textbf{xw}_j + \\textbf{b}_j )}\\right) $$\n",
    "\n",
    "\n",
    "$$L(z) = -log\\hspace{2px} \\left(  \\frac{e^{z_i}} {\\sum_{j=0}^{K}e^{z_j}} \\right) $$\n",
    "\n",
    "By applying the log division rule, this will be helpfull to avoid using the derivative quotient rule :\n",
    "\n",
    "$$L(z) = -\\left( z_i -log\\sum_{j=0}^{K}e^{z_j}  \\right)$$\n",
    "\n",
    "Now computing the derivative in respect to the class $i$ :\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial z_i} = -\\frac{\\partial }{\\partial z_i} \\left( z_i -log\\sum_{j=0}^{K}e^{z_j}  \\right) $$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial z_i} = \\frac{\\partial }{\\partial z_i} z_i - \\frac{\\partial }{\\partial z_i}log\\sum_{j=0}^{K}e^{z_j}   $$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial z_i} = 1 - log\\sum_{j=0}^{K}\\frac{\\partial }{\\partial z_i}e^{z_j} $$\n",
    "\n",
    "Note that the derivative for all the possibilities except for the desired class, its zero, therefore:\n",
    "\n",
    "$$ \\frac{\\partial }{\\partial z_i}e^{z_j} = \\left\\{ \\begin{array}{cl}\n",
    "0 & : \\ z_j \\neq z_i \\\\\n",
    "e^{z_i} & : \\ z_j  = z_i\n",
    "\\end{array} \\right. $$\n",
    "\n",
    "$$ \\frac{\\partial }{\\partial z_i}log\\left( \\sum_{j=0}^{K}e^{z_j} \\right) = \\frac{1}{\\sum_{j=0}^{K}e^{z_j} } e^{z_i} \n",
    " $$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial z_i} = 1 - \\frac{1}{\\sum_{j=0}^{K}e^{z_j} } e^{z_i} = 1 - z_i$$\n",
    "\n",
    "\n",
    "Now applying the chain rule: \n",
    "\n",
    "$$  \\frac{\\partial L}{\\partial w_i} = \\frac{\\partial L}{\\partial z_i} \\frac{\\partial z_i}{\\partial w_i}     $$\n",
    "\n",
    "$$  \\frac{\\partial L}{\\partial b_i} = \\frac{\\partial L}{\\partial z_i} \\frac{\\partial z_i}{\\partial b_i}     $$\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial w_i} =  \\left( 1 -  z_i \\right) \\textbf{x}  =  \\left( 1 - \\frac{\\text{exp}(\\textbf{xw}_i + \\textbf{b}_i )}{\\sum_{j=0}^{K}\\text{exp}(\\textbf{xw}_j + \\textbf{b}_j )}\\right)\\textbf{x}$$ \n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial b_i} =  \\left( 1 -  z_i \\right)  =  \\left( 1 - \\frac{\\text{exp}(\\textbf{xw}_i + \\textbf{b}_i )}{\\sum_{j=0}^{K}\\text{exp}(\\textbf{xw}_j + \\textbf{b}_j )}\\right)$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxRegression():\n",
    "    def __init__(self, p_size, n_classes, lr=0.001):\n",
    "        # self.w = np.random.rand(p_size, n_classes) * 0.01\n",
    "        self.w = np.random.rand(n_classes, p_size) * 0.01\n",
    "        self.b = np.random.rand(n_classes)\n",
    "        self.lr = lr\n",
    "        self.v_w = np.zeros_like(self.w)\n",
    "        self.v_b = np.zeros_like(self.b)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # return ( X @ self.w ) + self.b \n",
    "        return ( X @ self.w.T ) + self.b \n",
    "\n",
    "\n",
    "    def compute_probs(self, logits):\n",
    "        # softmax the network outputs\n",
    "        # return np.exp(logits) / np.sum(np.exp(logits))\n",
    "        exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))  # Stability trick\n",
    "        return exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
    "\n",
    "    def crossEntropy(self, y_hat, y):\n",
    "        b = - y * np.log(y_hat)\n",
    "        return b.sum(axis=-1) / len(y)\n",
    "\n",
    "    def loss_derivative(self, x, y_hat, y):\n",
    "        # return ( 1 - y_hat ).T @ x / len(x), np.sum(1 - y_hat, axis=0) / len(x)\n",
    "        return ( y - y_hat).T @ x / len(x), np.sum(y - y_hat, axis=0) / len(x)\n",
    "\n",
    "    \n",
    "    def gradient_descent(self, w_gradient, b_gradient):\n",
    "        \n",
    "        self.w -= self.lr * w_gradient\n",
    "        self.b -= self.lr * b_gradient\n",
    "\n",
    "    def stochastic_gradient_descent(self, X, y):\n",
    "        \n",
    "        random_indexes = np.arange(len(X))\n",
    "        np.random.shuffle(random_indexes)\n",
    "\n",
    "        X = X[random_indexes]\n",
    "        y = y[random_indexes]\n",
    "        \n",
    "        logits = self.forward(X)\n",
    "        y_hat = self.compute_probs(logits)\n",
    "        ce_loss = self.crossEntropy(y_hat, y)\n",
    "        w_gradient, b_gradient = self.loss_derivative(X, y_hat) \n",
    "\n",
    "        self.w -= self.lr * w_gradient.T\n",
    "        self.b -= self.lr * b_gradient.T\n",
    "\n",
    "        return ce_loss\n",
    "\n",
    "    def momentum_sgd(self, X, y, beta=0.9):\n",
    "        \n",
    "        random_indexes = np.arange(len(X))\n",
    "        np.random.shuffle(random_indexes)\n",
    "\n",
    "        X = X[random_indexes]\n",
    "        y = y[random_indexes]\n",
    "        \n",
    "        logits = self.forward(X)\n",
    "        y_hat = self.compute_probs(logits)\n",
    "        ce_loss = self.crossEntropy(y_hat, y)\n",
    "        w_gradient, b_gradient = self.loss_derivative(X, y_hat) \n",
    "\n",
    "        self.v_w = self.v_w * beta - self.lr * w_gradient.T\n",
    "        self.v_b = self.v_b * beta - self.lr * b_gradient.T\n",
    "\n",
    "        self.w -= self.v_w\n",
    "        self.b -= self.v_b \n",
    "\n",
    "        return ce_loss\n",
    "    \n",
    "    def optimize_model(self, opt_strategy, epochs, traindata, testdata):\n",
    "    \n",
    "        x_train, y_train = traindata\n",
    "        x_test, y_test = testdata\n",
    "\n",
    "        loss_record = []\n",
    "        opt_foo = self.gradient_descent        \n",
    "        # if opt_strategy == \"GD\":\n",
    "        #     opt_foo = self.gradient_descent\n",
    "        # elif opt_strategy == \"SGD\":\n",
    "        #     opt_foo = self.stochastic_gradient_descent\n",
    "\n",
    "        # elif opt_strategy == \"M-SGD\":\n",
    "        #     opt_foo = self.momentum_sgd\n",
    "\n",
    "        for e in range(epochs):\n",
    "\n",
    "            a = self.w\n",
    "\n",
    "            logits = self.forward(x_train)\n",
    "            y_hat = self.compute_probs(logits)\n",
    "            ce_loss = self.crossEntropy(y_hat, y_train)\n",
    "\n",
    "            w_gradient, b_gradient = self.loss_derivative(x_train, y_hat, y_train) \n",
    "            opt_foo(w_gradient, b_gradient)\n",
    "            \n",
    "            preds = np.argmax(self.compute_probs(self.forward(x_test)), axis=1)\n",
    "            print(preds[0])\n",
    "            print(\"acc =\", accuracy_score(y_test, preds) )\n",
    "\n",
    "            print(ce_loss.sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2.339057442251065\n",
      "1\n",
      "2.480771517140523\n",
      "1\n",
      "2.6646585406916494\n",
      "1\n",
      "2.9338911821738116\n",
      "1\n",
      "3.4047921211541436\n",
      "1\n",
      "4.385542898987815\n",
      "1\n",
      "6.444204902052498\n",
      "1\n",
      "9.57836398254892\n",
      "1\n",
      "13.045716774863395\n",
      "1\n",
      "16.566757671195973\n",
      "1\n",
      "20.09613885165557\n",
      "1\n",
      "23.626975369381153\n",
      "1\n",
      "27.158100924354663\n",
      "1\n",
      "30.689290739540063\n",
      "1\n",
      "34.22049626388289\n",
      "1\n",
      "37.751705947689096\n",
      "1\n",
      "41.282916809373724\n",
      "1\n",
      "44.81412802393851\n",
      "1\n",
      "48.3453393493052\n",
      "1\n",
      "51.87655071084495\n",
      "1\n",
      "55.40776208458106\n",
      "1\n",
      "58.938973462540794\n",
      "1\n",
      "62.47018484199615\n",
      "1\n",
      "66.0013962219911\n",
      "1\n",
      "69.53260760218382\n"
     ]
    }
   ],
   "source": [
    "clf = SoftmaxRegression(784, 10, lr=0.1)\n",
    "\n",
    "clf.optimize_model(\"GD\", 25, (x_train, y_train), (x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100, Loss: 1.5964\n",
      "Epoch 20/100, Loss: 1.2114\n",
      "Epoch 30/100, Loss: 1.0083\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MultinomialLogisticRegression:\n",
    "    def __init__(self, n_features, n_classes, learning_rate=0.01):\n",
    "        self.n_features = n_features\n",
    "        self.n_classes = n_classes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weights = np.random.randn(n_classes, n_features) * 0.01\n",
    "        self.biases = np.zeros(n_classes)\n",
    "    \n",
    "    def softmax(self, logits):\n",
    "        \"\"\"\n",
    "        Computes softmax probabilities for the input logits.\n",
    "        \"\"\"\n",
    "        exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))  # Stability trick\n",
    "        return exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Performs the forward pass, computing logits and softmax probabilities.\n",
    "        \"\"\"\n",
    "        logits = np.dot(X, self.weights.T) + self.biases\n",
    "        probabilities = self.softmax(logits)\n",
    "        return logits, probabilities\n",
    "    \n",
    "    def compute_loss(self, y_true, probabilities):\n",
    "        \"\"\"\n",
    "        Computes cross-entropy loss.\n",
    "        y_true: Ground truth labels (one-hot encoded).\n",
    "        probabilities: Softmax probabilities.\n",
    "        \"\"\"\n",
    "        n_samples = y_true.shape[0]\n",
    "        log_probs = np.log(probabilities + 1e-15)\n",
    "        loss = -np.sum(y_true * log_probs) / n_samples\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, X, y_true, probabilities):\n",
    "        \"\"\"\n",
    "        Computes gradients of weights and biases.\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        error = probabilities - y_true  # Gradient of loss with respect to logits\n",
    "        grad_weights = np.dot(error.T, X) / n_samples\n",
    "        grad_biases = np.sum(error, axis=0) / n_samples\n",
    "        return grad_weights, grad_biases\n",
    "    \n",
    "    def update_parameters(self, grad_weights, grad_biases):\n",
    "        \"\"\"\n",
    "        Updates weights and biases using gradient descent.\n",
    "        \"\"\"\n",
    "        self.weights -= self.learning_rate * grad_weights\n",
    "        self.biases -= self.learning_rate * grad_biases\n",
    "    \n",
    "    def train(self, X, y, epochs=100):\n",
    "        \"\"\"\n",
    "        Trains the model for a given number of epochs.\n",
    "        \"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            logits, probabilities = self.forward(X)\n",
    "            loss = self.compute_loss(y, probabilities)\n",
    "            grad_weights, grad_biases = self.backward(X, y, probabilities)\n",
    "            self.update_parameters(grad_weights, grad_biases)\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss:.4f}\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts class labels for input samples.\n",
    "        \"\"\"\n",
    "        _, probabilities = self.forward(X)\n",
    "        return np.argmax(probabilities, axis=1)\n",
    "\n",
    "\n",
    "# # Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Simulate a dataset\n",
    "#     np.random.seed(42)\n",
    "#     n_samples = 150\n",
    "#     n_features = 4\n",
    "#     n_classes = 3\n",
    "\n",
    "#     # Random input data\n",
    "#     X = np.random.rand(n_samples, n_features)\n",
    "\n",
    "#     # Random one-hot encoded labels\n",
    "#     y_indices = np.random.randint(0, n_classes, n_samples)\n",
    "#     y = np.eye(n_classes)[y_indices]  # One-hot encoding\n",
    "\n",
    "#     # Initialize and train the model\n",
    "model = MultinomialLogisticRegression(784, 10, learning_rate=0.1)\n",
    "model.train(x_train, y_train, epochs=100)\n",
    "\n",
    "#     # Predict and evaluate\n",
    "#     predictions = model.predict(X)\n",
    "#     accuracy = np.mean(predictions == y_indices)\n",
    "#     print(f\"Training Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
